{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import Model\n",
    "import efficientnet.keras as efn \n",
    "import librosa\n",
    "import librosa.display as display\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from PIL import Image\n",
    "from sklearn.utils import class_weight\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "\n",
    "from kapre.time_frequency import Melspectrogram\n",
    "from kapre.utils import Normalization2D\n",
    "from kapre.augmentation import AdditiveNoise\n",
    "from kapre.time_frequency import Spectrogram\n",
    "\n",
    "from python_speech_features import mfcc\n",
    "from mutagen.mp3 import MP3\n",
    "from audiomentations import Compose, AddGaussianNoise, TimeStretch, PitchShift, Shift\n",
    "\n",
    "from zipfile import ZipFile\n",
    "\n",
    "augmenter = Compose([\n",
    "    AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.02, p=0.5),\n",
    "    PitchShift(min_semitones=-2, max_semitones=2, p=0.5),\n",
    "    Shift(min_fraction=-0.5, max_fraction=0.5, p=0.5),\n",
    "])\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "#!rm -r train_data\n",
    "#!rm -r val_data\n",
    "#!rm -r models\n",
    "#!mkdir models\n",
    "\n",
    "# suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "SOUND_DIR = \"data/birdsong-recognition/train_audio/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-08-01 22:37:51--  https://s3.jp-tok.cloud-object-storage.appdomain.cloud/w251-models/efficientNet_20.zip\n",
      "Resolving s3.jp-tok.cloud-object-storage.appdomain.cloud (s3.jp-tok.cloud-object-storage.appdomain.cloud)... 162.133.118.49\n",
      "Connecting to s3.jp-tok.cloud-object-storage.appdomain.cloud (s3.jp-tok.cloud-object-storage.appdomain.cloud)|162.133.118.49|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 120919216 (115M) [application/zip]\n",
      "Saving to: ‘efficientNet_20.zip’\n",
      "\n",
      "efficientNet_20.zip 100%[===================>] 115.32M  5.38MB/s    in 29s     \n",
      "\n",
      "2020-08-01 22:38:20 (4.00 MB/s) - ‘efficientNet_20.zip’ saved [120919216/120919216]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# download and unzip model\n",
    "!wget https://s3.jp-tok.cloud-object-storage.appdomain.cloud/w251-models/efficientNet_20.zip\n",
    "    \n",
    "with ZipFile('efficientNet_20.zip', 'r') as zipObj:\n",
    "    zipObj.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model_path = \"efficientNet_20/\"\n",
    "model = tf.keras.models.load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "efficientnet-b3 (Model)      (None, 7, 7, 1536)        10783528  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 1536)              0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1536)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1536)              0         \n",
      "_________________________________________________________________\n",
      "softmax (Dense)              (None, 20)                30740     \n",
      "=================================================================\n",
      "Total params: 10,814,268\n",
      "Trainable params: 10,726,972\n",
      "Non-trainable params: 87,296\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# check model architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to perform inference\n",
    "def inference(model, signal, sr, classes, top=5):\n",
    "    \n",
    "    # transform audio signal to mel Spectrogram, \n",
    "    # pls note I down sample with 16000 Hz\n",
    "    S = Melspectrogram(n_dft=1024, n_hop=256, input_shape=(1, signal.shape[0]),\n",
    "                       padding='same', sr=sr, n_mels=224, fmin=1400, fmax=sr/2,\n",
    "                       power_melgram=2.0, return_decibel_melgram=True,\n",
    "                       trainable_fb=False, trainable_kernel=False)(signal.reshape(1, 1, -1)).numpy()\n",
    "    \n",
    "    S = S.reshape(S.shape[1], S.shape[2])\n",
    "    \n",
    "    # save tmp image with cmap = \"inferno\", which is the cmap used in training\n",
    "    matplotlib.image.imsave(\"tmp/inference\" + \".png\", S, cmap='inferno')\n",
    "    \n",
    "    # reload tmp image and convert to numpy array\n",
    "    img = Image.open('tmp/inference.png')\n",
    "    img = img.resize((224, 224), Image.ANTIALIAS)\n",
    "    melspec = np.array(img)\n",
    "    melspec = melspec[...,:3] / 255\n",
    "    \n",
    "    # model predictions\n",
    "    preds = model(melspec.reshape(1, 224, 224, 3)).numpy()\n",
    "    \n",
    "    # print top predictions\n",
    "    for pred in preds:\n",
    "        top_indices = pred.argsort()[-top:][::-1]\n",
    "        result = [[classes[i], pred[i]] for i in top_indices]\n",
    "        result.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "BIRDS = [\"amered\", \"annhum\", \"belkin1\", \"blugrb1\", \"brthum\", \n",
    "         \"cedwax\", \"commer\", \"gockin\", \"gryfly\", \"horlar\", \n",
    "         \"moudov\", \"olsfly\", \"pasfly\", \"semsan\", \"sposan\", \n",
    "         \"vigswa\", \"wewpew\", \"whbnut\", \"wilsni1\", \"yelwar\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference on sample audio (first 10s of audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['brthum', 1.0],\n",
       " ['wewpew', 9.0189474e-11],\n",
       " ['gockin', 9.897439e-12],\n",
       " ['gryfly', 3.3648594e-12],\n",
       " ['amered', 1.5433536e-12]]"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = \"data/birdsong-recognition/train_audio/brthum/XC132906.mp3\"\n",
    "signal, sr = librosa.load(file, sr=16000, duration=10)\n",
    "inference(model, signal, sr, BIRDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['whbnut', 0.9993772],\n",
       " ['semsan', 0.00040202064],\n",
       " ['wilsni1', 8.66735e-05],\n",
       " ['moudov', 3.9834027e-05],\n",
       " ['blugrb1', 2.6263682e-05]]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = \"data/birdsong-recognition/train_audio/whbnut/XC252947.mp3\"\n",
    "signal, sr = librosa.load(file, sr=16000, duration=10)\n",
    "inference(model, signal, sr, BIRDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['wilsni1', 0.9999999],\n",
       " ['semsan', 5.219136e-08],\n",
       " ['gryfly', 4.9135597e-08],\n",
       " ['vigswa', 5.4059752e-09],\n",
       " ['brthum', 2.617901e-09]]"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = \"data/birdsong-recognition/train_audio/wilsni1/XC147531.mp3\"\n",
    "signal, sr = librosa.load(file, sr=16000, duration=10)\n",
    "inference(model, signal, sr, BIRDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference on sample audio (random 10s of audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['brthum', 1.0],\n",
       " ['gockin', 1.4711168e-09],\n",
       " ['annhum', 2.6356056e-10],\n",
       " ['yelwar', 2.317562e-10],\n",
       " ['wewpew', 2.0799469e-10]]"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = \"data/birdsong-recognition/train_audio/brthum/XC132913.mp3\"\n",
    "signal, sr = librosa.load(file, sr=16000)\n",
    "\n",
    "start= int(np.random.uniform(0, len(signal) // sr - 10))\n",
    "signal = signal[sr*start:sr*(start+10)]\n",
    "inference(model, signal, sr, BIRDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['whbnut', 0.9293932],\n",
       " ['moudov', 0.029889416],\n",
       " ['belkin1', 0.02207578],\n",
       " ['blugrb1', 0.007992051],\n",
       " ['wilsni1', 0.0015644863]]"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = \"data/birdsong-recognition/train_audio/whbnut/XC290146.mp3\"\n",
    "signal, sr = librosa.load(file, sr=16000)\n",
    "\n",
    "start= int(np.random.uniform(0, len(signal) // sr - 10))\n",
    "signal = signal[sr*start:sr*(start+10)]\n",
    "inference(model, signal, sr, BIRDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['wilsni1', 0.9842981],\n",
       " ['horlar', 0.015352972],\n",
       " ['semsan', 0.00021072502],\n",
       " ['gryfly', 4.8712995e-05],\n",
       " ['vigswa', 2.3974026e-05]]"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = \"data/birdsong-recognition/train_audio/wilsni1/XC186352.mp3\"\n",
    "signal, sr = librosa.load(file, sr=16000)\n",
    "\n",
    "start= int(np.random.uniform(0, len(signal) // sr - 10))\n",
    "signal = signal[sr*start:sr*(start+10)]\n",
    "inference(model, signal, sr, BIRDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
