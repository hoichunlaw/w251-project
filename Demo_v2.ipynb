{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import general dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "%matplotlib inline\n",
    "\n",
    "# import augmentation and librosa dependencies\n",
    "import pydub\n",
    "from audiomentations import Compose, AddGaussianNoise, TimeStretch, PitchShift, Shift\n",
    "import multiprocessing as mp\n",
    "import librosa\n",
    "import librosa.display as display\n",
    "\n",
    "# import keras, tf and image depdencies\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import efficientnet.keras as efn \n",
    "from PIL import Image\n",
    "from sklearn.utils import class_weight\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# import kapre dependencies\n",
    "from kapre.time_frequency import Melspectrogram\n",
    "from kapre.utils import Normalization2D\n",
    "from kapre.augmentation import AdditiveNoise\n",
    "from kapre.time_frequency import Spectrogram\n",
    "\n",
    "# suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "SOUND_DIR = \"/project/data/birdsong-recognition/train_audio/\"\n",
    "SOUND_DIR_SUB = \"data/birdsong-recognition/train_audio/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################\n",
    "####  Function for creating Mel Spectrogram\n",
    "##########################################################\n",
    "\n",
    "def createMelSpectrogram(input_path, fileName, output_path, saveOrShow=0):\n",
    "    \n",
    "    # load sound signal\n",
    "    signal, sr = librosa.load(os.path.join(input_path, fileName), duration=10)\n",
    "    \n",
    "    # create Mel Spectrogram\n",
    "    S = Melspectrogram(n_dft=1024, \n",
    "                       n_hop=256,\n",
    "                       input_shape=(1, signal.shape[0]),\n",
    "                       padding='same', sr=sr, n_mels=224, fmin=1400, fmax=sr/2,\n",
    "                       power_melgram=2.0, return_decibel_melgram=True,\n",
    "                       trainable_fb=False, trainable_kernel=False)(signal.reshape(1, 1, -1)).numpy()\n",
    "    \n",
    "    S = S.reshape(S.shape[1], S.shape[2])\n",
    "    \n",
    "    if saveOrShow == 0:   \n",
    "        matplotlib.image.imsave(os.path.join(output_path, fileName.split(\".\")[0] + \".png\"), S)\n",
    "    else:\n",
    "        display.specshow(S, sr=sr)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################\n",
    "####  mp3 read and write methods. Credit to Stackoverflow\n",
    "####  user Basj for the starting point for these \n",
    "####  methods in his answer to:\n",
    "####  https://stackoverflow.com/questions/53633177/how-to-read-a-mp3-audio-file-into-a-numpy-array-save-a-numpy-array-to-mp3?noredirect=1&lq=1\n",
    "####\n",
    "####  The calling function passes a filename to\n",
    "####  be read or written\n",
    "####\n",
    "####  Note the normalization - MP3s are rarely anything\n",
    "####  other than 16 bit (signed 15 bit number), but there\n",
    "####  is a faint chance this wouldn't work.\n",
    "##########################################################\n",
    "\n",
    "def mp3_read(filename, normalized=False):\n",
    "    \"\"\"Read mp3 file to numpy array\"\"\"\n",
    "    with open(filename, \"rb\") as f:\n",
    "        a = pydub.AudioSegment.from_mp3(f)\n",
    "        y = np.array(a.get_array_of_samples()).astype('float64')\n",
    "        if a.channels == 2:\n",
    "            y = y.reshape((-1, 2))\n",
    "            \n",
    "        if normalized:\n",
    "            return a.frame_rate, np.float32(y) / 2**15\n",
    "        else:\n",
    "            return a.frame_rate, y\n",
    "    f.close()\n",
    "\n",
    "    \n",
    "def mp3_write(filename, sr, x, normalized=False):\n",
    "    \"\"\"Write numpy array to mp3 file\"\"\"\n",
    "    with open(filename, \"wb\") as f:\n",
    "        channels = 2 if (x.ndim == 2 and x.shape[1] == 2) else 1\n",
    "        if normalized:  # normalized array - each item should be a float in [-1, 1)\n",
    "            y = np.int16(x * 2 ** 15)\n",
    "        else:\n",
    "            y = np.int16(x)\n",
    "        clip = pydub.AudioSegment(y.tobytes(), frame_rate=sr, sample_width=2, channels=channels)\n",
    "        clip.export(f, format=\"mp3\", bitrate=f\"{sr * 2**16}\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################\n",
    "####  Core function for augmentation of a single file \n",
    "##########################################################\n",
    "\n",
    "def augment(source_file,dest_file,augmentation):\n",
    "    \"\"\"\n",
    "    augment() - used for the multiprocessing\n",
    "    later on to accelerate the creation of all the\n",
    "    augmentations as fast as possible\n",
    "    \n",
    "    source_file:  path and filename of source sample\n",
    "    dest_file:    path and filename of output sample\n",
    "    augmentation: an instance of audiomentations.Compose\n",
    "                  containing the augmentation(s)    \n",
    "    \n",
    "    If passed a stereo file, it will create two mono files\n",
    "    from the two channels. If the passed file does not exist,\n",
    "    then we look for it in the stereo subdirectory. If it\n",
    "    does exist, it gets moved there after the split.\n",
    "    \"\"\"\n",
    "    source_file0 = f\"{source_file[:-4]}.0{source_file[-4:]}\"\n",
    "    source_file1 = f\"{source_file[:-4]}.1{source_file[-4:]}\"\n",
    "    dest_file0 = f\"{dest_file[:-4]}.0{dest_file[-4:]}\"\n",
    "    dest_file1 = f\"{dest_file[:-4]}.1{dest_file[-4:]}\"\n",
    "\n",
    "    if(os.path.isfile(source_file0) and os.path.isfile(source_file1) and not(os.path.isfile(source_file))):\n",
    "        # the stereo split has already been done, so augment each of \n",
    "        # the files. \n",
    "        augment(source_file0,dest_file0,augmentation)\n",
    "        augment(source_file1,dest_file1,augmentation)\n",
    "\n",
    "    else:\n",
    "        # The file is either mono already or not yet split\n",
    "        # Read information from the mp3 file\n",
    "        try:            \n",
    "            sample_rate, chirp = mp3_read(source_file)\n",
    "        except:\n",
    "            print(f\"Problem encountered reading in {os.path.basename(source_file)}\" )\n",
    "            errors.append(source_file)\n",
    "            return\n",
    "\n",
    "            \n",
    "\n",
    "        # If the chirp file is in stereo (two column array)\n",
    "        # then create two mono files from the two channels\n",
    "        # and run the augmentation on each of them.\n",
    "\n",
    "        if (len(chirp.shape) == 2 and chirp.shape[1] == 2):\n",
    "            # Recording is in stereo, splitting into two mono files\n",
    "\n",
    "            # write the split mono files from the two channels\n",
    "            mp3_write(source_file0, sample_rate, chirp[:,0], normalized=False)\n",
    "            mp3_write(source_file1, sample_rate, chirp[:,1], normalized=False)\n",
    "\n",
    "            # run augmentation on each\n",
    "            augment(source_file0,dest_file0,augmentation)\n",
    "            augment(source_file1,dest_file1,augmentation)          \n",
    "            \n",
    "            # move the stereo version of the file to a new location\n",
    "            if not os.path.exists(os.path.dirname(source_file) + \"/stereo/\"): \n",
    "                os.mkdir(os.path.dirname(source_file) + \"/stereo/\")\n",
    "                \n",
    "            os.rename(source_file, f\"{os.path.dirname(source_file)}/stereo/{os.path.basename(source_file)}\")\n",
    "         \n",
    "        else:\n",
    "            try:\n",
    "                # file is mono, and we can process directly\n",
    "                \n",
    "                if( not(os.path.isfile(dest_file)) or os.path.getsize(dest_file) == 0 ):\n",
    "                    new_chirp = augmentation(chirp, sample_rate=sample_rate) \n",
    "                    # Write out the new chirp audio to an mp3\n",
    "                    mp3_write(dest_file, sample_rate, new_chirp, normalized=False)\n",
    "            except:\n",
    "                print(f\"Problem encountered processing {os.path.basename(dest_file)}\" )\n",
    "                errors.append(dest_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################\n",
    "####  Process a single directory of bird sounds\n",
    "###########################################################\n",
    "\n",
    "def augment_bird(bird):\n",
    "    \n",
    "    # add a random pause to avoid messages overwriting each other\n",
    "    time.sleep(np.random.uniform(0,5))\n",
    "    start_time = time.time()\n",
    "\n",
    "    input_dir = os.path.join(SOUND_DIR_SUB, bird)\n",
    "    # Ignore augmented files starting with \"A\"\n",
    "    chirps = [f for f in os.listdir(input_dir) if re.match(r'X.*\\.mp3', f)]\n",
    "    \n",
    "    print(f\"\\nCreating augmented samples from {len(chirps)} files for {bird}\")\n",
    "\n",
    "    # only enhance files that start with X - the generated files start with A\n",
    "    for chirp in chirps:\n",
    "    \n",
    "        chirp_file = os.path.join(input_dir, chirp)\n",
    "\n",
    "        # Augmentation 1 - random time shift + or - up to 0.5s\n",
    "        augmentation = Compose([Shift(min_fraction=-0.5, max_fraction=0.5, p=0.5)])\n",
    "        dest_file =  os.path.join(input_dir, f\"A.{chirp[:-4]}.ts0.5{chirp[-4:]}\")\n",
    "        augment(chirp_file, dest_file, augmentation)\n",
    "\n",
    "        # Augmentation 2 - random frequency shift up or down by up to 2 semitones\n",
    "        augmentation = Compose([PitchShift(min_semitones=-2, max_semitones=2, p=0.5)])\n",
    "        dest_file =  os.path.join(input_dir, f\"A.{chirp[:-4]}.fs2{chirp[-4:]}\")\n",
    "        augment(chirp_file, dest_file, augmentation)\n",
    "\n",
    "        # Augmentation 3 - random time expansion / contraction up to 20%\n",
    "        augmentation = Compose([TimeStretch(min_rate=0.8, max_rate=1.2, p=0.5)])\n",
    "        dest_file =  os.path.join(input_dir, f\"A.{chirp[:-4]}.ex20{chirp[-4:]}\")\n",
    "        augment(chirp_file, dest_file, augmentation)\n",
    "\n",
    "        # Augmentation 4 - add gaussian noise between 0.001 and 0.015 amplitude\n",
    "        augmentation = Compose([AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=0.5)])\n",
    "        dest_file =  os.path.join(input_dir, f\"A.{chirp[:-4]}.gn015{chirp[-4:]}\")\n",
    "        augment(chirp_file, dest_file, augmentation)\n",
    "\n",
    "    duration = time.time() - start_time\n",
    "\n",
    "    print(f\"Augmentation complete in {time.strftime('%H:%M:%S', time.gmtime(duration))} \" +\\\n",
    "          f\"for {os.path.basename(input_dir)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################\n",
    "####  Main function to augment all birds\n",
    "###########################################################\n",
    "\n",
    "def augment_multiple_birds(bird_list=[\"*\"]):\n",
    "    completed_birds = 0\n",
    "    errors = []\n",
    "    np.random.seed(1234)\n",
    "    \n",
    "    main_start = time.time()\n",
    "    \n",
    "    print(\"Start Time: \", time.strftime('%H:%M:%S', time.gmtime(main_start)))\n",
    "    \n",
    "    if bird_list[0] != \"*\":\n",
    "        birds = bird_list\n",
    "    else:\n",
    "        birds = os.listdir(SOUND_DIR_SUB)\n",
    "    \n",
    "    birds.sort()\n",
    "    total_birds = len(birds)\n",
    "\n",
    "    print(birds)\n",
    "    \n",
    "    # comment out the next line and uncomment the following one if running\n",
    "    # on non-hyperthreaded CPU cores. Assigns one thread per physical core\n",
    "    # reserving one core for OS, IO, compression etc. when using cloud\n",
    "    # object storage, or override with your own number.\n",
    "    # I ran this on a 32 core processor.\n",
    "    \n",
    "    threads = int((mp.cpu_count() /2) - 1)\n",
    "    # threads = int(mp.cpu_count() -2)\n",
    "    # threads = 48\n",
    "\n",
    "    # Handle single-core machines\n",
    "    if (threads < 1): threads = 1\n",
    "    print(f\"Launching {threads} threads\")\n",
    "    pool = mp.Pool(threads)\n",
    "    \n",
    "    pool.map(augment_bird, birds)\n",
    "    \n",
    "    main_end = time.time()\n",
    "    duration = main_end - main_start\n",
    "    \n",
    "    print(time.strftime('%H:%M:%S', time.gmtime(duration)))\n",
    "    print(\"End Time: \", time.strftime('%H:%M:%S', time.gmtime(main_end)))\n",
    "\n",
    "    print(f\"Completed all augmentations in {time.strftime('%H:%M:%S', time.gmtime(duration))}\")\n",
    "    print()\n",
    "    print(f\"A total of {len(errors)} occurred, errors encountered generating:\")\n",
    "    for error in errors:\n",
    "        print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################\n",
    "####  Gets headless model URl for transfer learning\n",
    "####  See this for a complete list of models\n",
    "####  https://tfhub.dev/s?fine-tunable=yes&module-type=image-feature-vector&tf-version=tf2&q=tf2\n",
    "###########################################################\n",
    "\n",
    "model_url_mapping = {\n",
    "    \"tf2_mobilenet_v2\": \"https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/\",\n",
    "    \"imgnet_mobilenet_v2_140_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v2_140_224/feature_vector/\",\n",
    "    \n",
    "    \"imgnet_resnet_v2_50\": \"https://tfhub.dev/google/imagenet/resnet_v2_50/feature_vector/\",\n",
    "    \"imgnet_resnet_v2_152\" : \"https://tfhub.dev/google/imagenet/resnet_v2_152/feature_vector/\",\n",
    "    \"imgnet_inception_resnet_v2\" : \"https://tfhub.dev/google/imagenet/inception_resnet_v2/feature_vector/\",\n",
    "    \n",
    "    \"tf2_inception_v3\": \"https://tfhub.dev/google/tf2-preview/inception_v3/feature_vector/\",\n",
    "    \"imgnet_inception_v3\": \"https://tfhub.dev/google/imagenet/inception_v3/feature_vector/\",\n",
    "    \n",
    "    \"imgnet_pnasnet_large\" : \"https://tfhub.dev/google/imagenet/pnasnet_large/feature_vector/\",\n",
    "    \"imgnet_nasnet_mobile\" : \"https://tfhub.dev/google/imagenet/nasnet_mobile/feature_vector/\"\n",
    "}\n",
    "\n",
    "def get_model_url(model_name, model_iteration=4):\n",
    "    return model_url_mapping[model_name] + str(model_iteration)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Mel-Spectrogram of an Audio Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bird: aldfly\n",
    "createMelSpectrogram(SOUND_DIR + \"aldfly\", \"XC134874.mp3\", \"\", 1)\n",
    "createMelSpectrogram(SOUND_DIR + \"aldfly\", \"XC16967.mp3\", \"\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bird: ameavo\n",
    "createMelSpectrogram(SOUND_DIR + \"ameavo\", \"XC133080.mp3\", \"\", 1)\n",
    "createMelSpectrogram(SOUND_DIR + \"ameavo\", \"XC139829.mp3\", \"\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bird: amebit\n",
    "createMelSpectrogram(SOUND_DIR + \"amebit\", \"XC141316.mp3\", \"\", 1)\n",
    "createMelSpectrogram(SOUND_DIR + \"amebit\", \"XC184876.mp3\", \"\", 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select top 20 birds for training\n",
    "BIRDS = os.listdir(SOUND_DIR)[0:20]\n",
    "\n",
    "# set common settings and params\n",
    "IM_SIZE = (224, 224, 3)\n",
    "BATCH_SIZE = 16\n",
    "TRAINABLE = True\n",
    "MODEL_NAME = \"imgnet_resnet_v2_152\"\n",
    "EPOCHS = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augment Bird Audio Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify bird list that needs augmentation\n",
    "# For all birds, leave it without parameters\n",
    "augment_multiple_birds([\"aldfly\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Mel-Spectrograms for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of birds\n",
    "#BIRDS = [\"aldfly\", \"ameavo\", \"amebit\", \"amecro\", \"amegfi\", \"amekes\", \"amepip\", \"amered\", \"amerob\", \"amewig\"]\n",
    "\n",
    "train_folder = \"train_data_decibel_20/\"\n",
    "val_folder = \"val_data_decibel_20/\"\n",
    "\n",
    "if not os.path.exists(train_folder): os.mkdir(train_folder)\n",
    "if not os.path.exists(val_folder): os.mkdir(val_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################\n",
    "####  This will take a long time to run\n",
    "###########################################################\n",
    "\n",
    "# create train and val spectrogram\n",
    "np.random.seed(1234)\n",
    "for bird in tqdm(BIRDS):\n",
    "    INPUT_DIR = os.path.join(SOUND_DIR, bird)\n",
    "    TRAIN_DIR = os.path.join(train_folder, bird)\n",
    "    VAL_DIR = os.path.join(val_folder, bird)\n",
    "    \n",
    "    # create folders\n",
    "    if not(os.path.exists(TRAIN_DIR)) and not(os.path.exists(VAL_DIR)): \n",
    "        \n",
    "        os.mkdir(TRAIN_DIR)\n",
    "        os.mkdir(VAL_DIR)\n",
    "\n",
    "        # split into train and val set\n",
    "        for f in os.listdir(INPUT_DIR):\n",
    "            rand = np.random.randint(0, 10)\n",
    "            if f[0] != \".\":\n",
    "                if rand <= 7: \n",
    "                    createMelSpectrogram(INPUT_DIR, f, TRAIN_DIR)\n",
    "                else:\n",
    "                    createMelSpectrogram(INPUT_DIR, f, VAL_DIR)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Training and Test Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data generator - uses the ImageDataGenerator from Keras\n",
    "datagen = ImageDataGenerator(preprocessing_function=None,\n",
    "                             rescale=1/255,\n",
    "                             width_shift_range=0.2,\n",
    "                             height_shift_range=0.2,\n",
    "                             shear_range=0.2,\n",
    "                             zoom_range=0.1,\n",
    "                             fill_mode='nearest')\n",
    "\n",
    "# generate train batches\n",
    "train_batches = datagen.flow_from_directory(train_folder,\n",
    "                                            classes=BIRDS, \n",
    "                                            target_size=IM_SIZE[0:2], \n",
    "                                            class_mode='categorical', \n",
    "                                            shuffle=True, batch_size=BATCH_SIZE)\n",
    "\n",
    "# generate validation batches\n",
    "val_batches = datagen.flow_from_directory(val_folder,\n",
    "                                          classes=BIRDS, \n",
    "                                          target_size=IM_SIZE[0:2], \n",
    "                                          class_mode='categorical', \n",
    "                                          shuffle=False, batch_size=1)\n",
    "\n",
    "# estimate class weights for unbalanced datasets.\n",
    "class_weights = class_weight.compute_class_weight('balanced', \n",
    "                                                  np.unique(train_batches.classes), \n",
    "                                                  train_batches.classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model and Train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the below URL gets the headless model from tf2 hub\n",
    "feature_extractor_url = get_model_url(MODEL_NAME, 4)\n",
    "\n",
    "# create feature extractor from prebuilt model using Keras\n",
    "feature_extractor_layer = hub.KerasLayer(feature_extractor_url,\n",
    "                                         input_shape=IM_SIZE)\n",
    "\n",
    "# informational\n",
    "for image_batch, label_batch in train_batches:\n",
    "  print(\"Image batch shape: \", image_batch.shape)\n",
    "  print(\"Label batch shape: \", label_batch.shape)\n",
    "  break\n",
    "\n",
    "# informational\n",
    "feature_batch = feature_extractor_layer(image_batch)\n",
    "print(feature_batch.shape)\n",
    "\n",
    "# we can choose to lock the layers except the top layer (or not)\n",
    "feature_extractor_layer.trainable = TRAINABLE\n",
    "\n",
    "# attach a classification head and create the model object\n",
    "model = tf.keras.Sequential([\n",
    "  feature_extractor_layer,\n",
    "  tf.keras.layers.Dense(len(BIRDS), activation='softmax', name='softmax')\n",
    "])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = tf.keras.optimizers.Adam(), \n",
    "              loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True), \n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollectBatchStats(tf.keras.callbacks.Callback):\n",
    "  def __init__(self):\n",
    "    self.batch_losses = []\n",
    "    self.batch_acc = []\n",
    "\n",
    "  def on_train_batch_end(self, batch, logs=None):\n",
    "    self.batch_losses.append(logs['loss'])\n",
    "    self.batch_acc.append(logs['accuracy'])\n",
    "    self.model.reset_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_stats_callback = CollectBatchStats()\n",
    "\n",
    "history = model.fit_generator(train_batches, \n",
    "                              epochs = EPOCHS,\n",
    "                              validation_data = val_batches,\n",
    "                              steps_per_epoch = int(len(train_batches.classes)/BATCH_SIZE)+1,\n",
    "                              validation_steps = len(val_batches.classes),\n",
    "                              callbacks = [batch_stats_callback],\n",
    "                              class_weight = {i:class_weights[i] for i in range(len(BIRDS))})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.ylim([0,10])\n",
    "plt.plot(batch_stats_callback.batch_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.ylim([0,1])\n",
    "plt.plot(batch_stats_callback.batch_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time.ctime().replace(\" \",\"_\").replace(\":\",\"_\")\n",
    "\n",
    "export_path = \"models/{}/{}\".format(MODEL_NAME, t)\n",
    "model.save(export_path, save_format='tf')\n",
    "\n",
    "export_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reloaded = tf.keras.models.load_model(export_path)\n",
    "print(\"Accuracy on val data for model: {}\".format(MODEL_NAME))\n",
    "reloaded.evaluate(val_batches, steps=len(val_batches.classes))[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Against Original EfficientNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The original model was included as a part of this blog\n",
    "# https://towardsdatascience.com/sound-based-bird-classification-965d0ecacb2b\n",
    "# Here, we also use a similar headless model approach, because \"include_top\" is false\n",
    "net = efn.EfficientNetB3(include_top=False, weights=\"imagenet\", input_tensor=None, input_shape=IM_SIZE)\n",
    "net.trainable = TRAINABLE\n",
    "\n",
    "x = net.output\n",
    "\n",
    "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "x = tf.keras.layers.Dropout(0.5)(x)\n",
    "\n",
    "output_layer = tf.keras.layers.Dense(len(BIRDS), activation='softmax', name='softmax')(x)\n",
    "net_final = tf.keras.Model(inputs=net.input, outputs=output_layer)\n",
    "\n",
    "net_final.compile(optimizer=tf.keras.optimizers.Adam(), \n",
    "                  loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "checkpoint_path = 'models/efficientnet/efficientnet_checkpoint.h5'\n",
    "#net_final.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ModelCheck = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, monitor='val_loss', verbose=0, \n",
    "                             save_best_only=True, save_weights_only=True, mode='auto')\n",
    "\n",
    "net_final.reset_states()\n",
    "net_final.fit_generator(train_batches, \n",
    "                        validation_data = val_batches,\n",
    "                        steps_per_epoch = int(len(train_batches.classes)/BATCH_SIZE)+1,\n",
    "                        validation_steps = len(val_batches.classes),\n",
    "                        epochs = EPOCHS, \n",
    "                        callbacks = [ModelCheck],\n",
    "                        class_weight = {i:class_weights[i] for i in range(len(BIRDS))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_final.load_weights(checkpoint_path)\n",
    "print(\"Accuracy on val data with efficientnet\")\n",
    "net_final.evaluate(val_batches, steps=len(val_batches.classes))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
